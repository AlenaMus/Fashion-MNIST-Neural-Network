{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fashion MNIST Clothes Classification Neural Network\n",
    "\n",
    "## Project Overview\n",
    "\n",
    "This notebook implements a **fully connected neural network** to classify clothing items from the Fashion MNIST dataset.\n",
    "\n",
    "### What is Fashion MNIST?\n",
    "\n",
    "Fashion MNIST is a dataset of clothing images created by Zalando as a modern replacement for the classic MNIST handwritten digits dataset. It contains:\n",
    "\n",
    "- **60,000 training images**\n",
    "- **10,000 test images**\n",
    "- **28Ã—28 pixel grayscale images** (single channel)\n",
    "- **10 clothing categories**: T-shirt/top, Trouser, Pullover, Dress, Coat, Sandal, Shirt, Sneaker, Bag, Ankle boot\n",
    "\n",
    "### Our Approach\n",
    "\n",
    "We'll build a **Dense (Fully Connected) Neural Network** with the following architecture:\n",
    "\n",
    "```\n",
    "Input (784) â†’ Dense(512, ReLU) â†’ Dropout(0.2) â†’ Dense(256, ReLU) â†’ Dropout(0.2) â†’ Dense(10, Softmax)\n",
    "```\n",
    "\n",
    "**Key Features:**\n",
    "- Multi-layer perceptron (MLP) architecture\n",
    "- ReLU activation for hidden layers\n",
    "- Dropout regularization to prevent overfitting\n",
    "- Softmax output for multi-class classification\n",
    "- Adam optimizer with Categorical Crossentropy loss\n",
    "\n",
    "**Target Performance:** 85-90% accuracy on test set\n",
    "\n",
    "### Notebook Structure\n",
    "\n",
    "1. Environment setup and GPU configuration\n",
    "2. Data loading and exploration\n",
    "3. Data preprocessing (normalization, flattening, one-hot encoding)\n",
    "4. Neural network architecture design\n",
    "5. Model compilation and training\n",
    "6. Performance evaluation and visualization\n",
    "7. Prediction on custom images\n",
    "\n",
    "Let's get started! ðŸš€"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Imports & GPU Configuration\n",
    "\n",
    "We begin by importing all necessary libraries and configuring the GPU for accelerated training.\n",
    "\n",
    "**Key Libraries:**\n",
    "- **TensorFlow/Keras**: Deep learning framework for building and training neural networks\n",
    "- **NumPy**: Numerical computing for array operations\n",
    "- **Matplotlib/Seaborn**: Visualization for graphs and confusion matrix\n",
    "- **scikit-learn**: Metrics computation (confusion matrix)\n",
    "- **Pillow**: Image loading and processing\n",
    "\n",
    "**GPU Acceleration:**\n",
    "- TensorFlow automatically detects and uses CUDA-compatible NVIDIA GPUs\n",
    "- Training on GPU is 10-50x faster than CPU for neural networks\n",
    "- If no GPU is available, training will fall back to CPU (slower but functional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "=============================================================================\n",
    "SECTION 1: IMPORTS AND GPU CONFIGURATION\n",
    "=============================================================================\n",
    "Import all required libraries and verify GPU availability.\n",
    "TensorFlow will automatically use GPU if available (CUDA-compatible NVIDIA GPU required).\n",
    "\"\"\"\n",
    "\n",
    "# --- Standard Libraries ---\n",
    "import numpy as np                          # Numerical computing and array operations\n",
    "import random                               # Random number generation for seed setting\n",
    "import os                                   # Operating system interface\n",
    "\n",
    "# --- TensorFlow & Keras ---\n",
    "import tensorflow as tf                     # Deep learning framework\n",
    "from tensorflow.keras.models import Sequential    # Sequential model API\n",
    "from tensorflow.keras.layers import Dense, Dropout # Neural network layers\n",
    "from tensorflow.keras.datasets import fashion_mnist # Fashion MNIST dataset\n",
    "from tensorflow.keras.utils import to_categorical   # One-hot encoding utility\n",
    "\n",
    "# --- Visualization ---\n",
    "import matplotlib.pyplot as plt             # Plotting and visualization\n",
    "import seaborn as sns                       # Statistical visualization (for confusion matrix)\n",
    "\n",
    "# --- Metrics ---\n",
    "from sklearn.metrics import confusion_matrix # Confusion matrix computation\n",
    "\n",
    "# --- Image Processing ---\n",
    "from PIL import Image                       # Image loading and processing\n",
    "\n",
    "# --- Reproducibility ---\n",
    "# Setting random seeds ensures we get the same results every run\n",
    "# This is important for debugging and comparing experiments\n",
    "SEED = 42\n",
    "random.seed(SEED)                           # Python random seed\n",
    "np.random.seed(SEED)                        # NumPy random seed\n",
    "tf.random.set_seed(SEED)                    # TensorFlow random seed\n",
    "\n",
    "# --- GPU Configuration ---\n",
    "# Check if a GPU is available for accelerated training\n",
    "# TensorFlow automatically uses GPU when available\n",
    "print(\"=\"*60)\n",
    "print(\"GPU CONFIGURATION\")\n",
    "print(\"=\"*60)\n",
    "print(f\"TensorFlow Version: {tf.__version__}\")\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    print(f\"\\nâœ“ GPU Available: {len(gpus)} device(s) detected\")\n",
    "    for gpu in gpus:\n",
    "        print(f\"  â†’ {gpu.name}\")\n",
    "    # Allow memory growth to prevent TensorFlow from allocating all GPU memory at once\n",
    "    for gpu in gpus:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    print(\"\\n  Memory growth enabled for efficient GPU usage\")\n",
    "else:\n",
    "    print(\"\\nâš  WARNING: No GPU detected!\")\n",
    "    print(\"  Training will run on CPU (significantly slower)\")\n",
    "    print(\"  For GPU support, ensure CUDA toolkit and cuDNN are installed\")\n",
    "\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Define Class Names\n",
    "\n",
    "Fashion MNIST contains **10 mutually exclusive clothing categories**. Each image belongs to exactly one class.\n",
    "\n",
    "The labels are encoded as integers (0-9), which we map to human-readable names:\n",
    "\n",
    "| Label | Class Name | Description |\n",
    "|-------|------------|-------------|\n",
    "| 0 | T-shirt/top | Short-sleeved upper body garment |\n",
    "| 1 | Trouser | Long lower body garment |\n",
    "| 2 | Pullover | Long-sleeved upper body garment (no buttons) |\n",
    "| 3 | Dress | One-piece garment |\n",
    "| 4 | Coat | Heavy outer garment |\n",
    "| 5 | Sandal | Open-toed footwear |\n",
    "| 6 | Shirt | Button-up upper body garment |\n",
    "| 7 | Sneaker | Casual closed-toe footwear |\n",
    "| 8 | Bag | Handbag/purse |\n",
    "| 9 | Ankle boot | Short boot footwear |\n",
    "\n",
    "These class names will be used for visualization and interpretation throughout the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "=============================================================================\n",
    "SECTION 2: CLASS LABEL DEFINITIONS\n",
    "=============================================================================\n",
    "Fashion MNIST contains 10 categories of clothing items.\n",
    "Each image belongs to exactly one of these classes (mutually exclusive).\n",
    "\"\"\"\n",
    "\n",
    "# Class names mapping - index corresponds to label number (0-9)\n",
    "# These are the 10 clothing categories in the Fashion MNIST dataset\n",
    "class_names = [\n",
    "    'T-shirt/top',  # 0 - Short-sleeved upper body garment\n",
    "    'Trouser',       # 1 - Long lower body garment\n",
    "    'Pullover',      # 2 - Long-sleeved upper body garment (no buttons)\n",
    "    'Dress',         # 3 - One-piece garment\n",
    "    'Coat',          # 4 - Heavy outer garment\n",
    "    'Sandal',        # 5 - Open-toed footwear\n",
    "    'Shirt',         # 6 - Button-up upper body garment\n",
    "    'Sneaker',       # 7 - Casual closed-toe footwear\n",
    "    'Bag',           # 8 - Handbag/purse\n",
    "    'Ankle boot'     # 9 - Short boot footwear\n",
    "]\n",
    "\n",
    "print(f\"Number of classes: {len(class_names)}\")\n",
    "print(\"\\nClass mapping:\")\n",
    "for i, name in enumerate(class_names):\n",
    "    print(f\"  {i}: {name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Load Dataset\n",
    "\n",
    "We load the Fashion MNIST dataset directly from Keras, which provides convenient access to many popular datasets.\n",
    "\n",
    "**Dataset Split:**\n",
    "- **Training set**: 60,000 images (used to train the neural network)\n",
    "- **Test set**: 10,000 images (held out for final evaluation on unseen data)\n",
    "\n",
    "**Image Format:**\n",
    "- Each image is **28Ã—28 pixels**\n",
    "- **Grayscale** (single channel, not RGB)\n",
    "- Pixel values range from **0 (black) to 255 (white)**\n",
    "- Data type: `uint8` (unsigned 8-bit integer)\n",
    "\n",
    "**Labels:**\n",
    "- Integer values from **0 to 9**\n",
    "- Correspond to the 10 clothing classes defined above\n",
    "\n",
    "The dataset is automatically downloaded the first time you run this cell (may take a few seconds)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "=============================================================================\n",
    "SECTION 3: DATA LOADING\n",
    "=============================================================================\n",
    "Load the Fashion MNIST dataset directly from Keras.\n",
    "The dataset is automatically split into training and test sets:\n",
    "  - Training set: 60,000 images (used to train the neural network)\n",
    "  - Test set: 10,000 images (used to evaluate performance on unseen data)\n",
    "Each image is 28x28 pixels in grayscale (single channel, values 0-255).\n",
    "\"\"\"\n",
    "\n",
    "# Load Fashion MNIST dataset\n",
    "# Returns two tuples: (training data, training labels), (test data, test labels)\n",
    "(X_train_raw, y_train_raw), (X_test_raw, y_test_raw) = fashion_mnist.load_data()\n",
    "\n",
    "# Display dataset information\n",
    "print(\"=\"*60)\n",
    "print(\"DATASET LOADED SUCCESSFULLY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nTraining set:\")\n",
    "print(f\"  Images shape: {X_train_raw.shape}\")      # (60000, 28, 28)\n",
    "print(f\"  Labels shape: {y_train_raw.shape}\")       # (60000,)\n",
    "print(f\"  Pixel value range: [{X_train_raw.min()}, {X_train_raw.max()}]\")\n",
    "\n",
    "print(f\"\\nTest set:\")\n",
    "print(f\"  Images shape: {X_test_raw.shape}\")        # (10000, 28, 28)\n",
    "print(f\"  Labels shape: {y_test_raw.shape}\")        # (10000,)\n",
    "print(f\"  Pixel value range: [{X_test_raw.min()}, {X_test_raw.max()}]\")\n",
    "\n",
    "print(f\"\\nImage dimensions: {X_train_raw.shape[1]}x{X_train_raw.shape[2]} pixels\")\n",
    "print(f\"Data type: {X_train_raw.dtype}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5: Preview Images Function\n",
    "\n",
    "Before training our neural network, it's important to **visually inspect the data** to understand what we're working with.\n",
    "\n",
    "This function displays images at **indices 6, 7, 8, and 9** from the training set in a **2Ã—2 grid**. Visual inspection helps us:\n",
    "\n",
    "1. **Verify data loaded correctly** - ensure images are not corrupted\n",
    "2. **Understand the challenge** - see how diverse the clothing items are\n",
    "3. **Check image quality** - 28Ã—28 is quite low resolution!\n",
    "4. **Inspect label correctness** - confirm labels match the images\n",
    "\n",
    "**Why indices 6-9?**\n",
    "- Arbitrary choice to show variety\n",
    "- You can modify the `indices` parameter to view any images you want\n",
    "- Example: `preview_dataset_images(X_train_raw, y_train_raw, indices=[0, 100, 500, 1000])`\n",
    "\n",
    "The images are displayed in grayscale with their index number and class label shown in the title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "=============================================================================\n",
    "SECTION 4: DATA VISUALIZATION - PREVIEW FUNCTION\n",
    "=============================================================================\n",
    "Visualize sample images from the dataset to understand what the data looks like.\n",
    "This function displays images at indices 6, 7, 8, 9 in a 2x2 grid.\n",
    "Visual inspection helps verify data loaded correctly and understand the challenge.\n",
    "\"\"\"\n",
    "\n",
    "def preview_dataset_images(images, labels, indices=[6, 7, 8, 9]):\n",
    "    \"\"\"\n",
    "    Display a 2x2 grid of images from the dataset with their class labels.\n",
    "    \n",
    "    This function helps visualize the raw data before preprocessing,\n",
    "    allowing us to verify the data loaded correctly and understand\n",
    "    the visual characteristics of each clothing category.\n",
    "    \n",
    "    Args:\n",
    "        images (np.array): Image data array of shape (N, 28, 28)\n",
    "        labels (np.array): Label array of shape (N,) with values 0-9\n",
    "        indices (list): List of 4 image indices to display (default: [6,7,8,9])\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(8, 8))\n",
    "    fig.suptitle('Fashion MNIST - Sample Images (Indices 6-9)', \n",
    "                 fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Flatten the 2x2 grid of axes for easy iteration\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i, idx in enumerate(indices):\n",
    "        # Display the image in grayscale\n",
    "        axes[i].imshow(images[idx], cmap='gray')\n",
    "        # Set title with index and class name\n",
    "        axes[i].set_title(\n",
    "            f'Index: {idx} | Label: {labels[idx]} ({class_names[labels[idx]]})',\n",
    "            fontsize=11\n",
    "        )\n",
    "        # Remove axis ticks for cleaner appearance\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Call the preview function to display images 6, 7, 8, 9\n",
    "print(\"Previewing images at indices 6, 7, 8, 9:\")\n",
    "preview_dataset_images(X_train_raw, y_train_raw, indices=[6, 7, 8, 9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 6: Data Preprocessing\n",
    "\n",
    "Raw data needs to be transformed into a format suitable for neural network training. We perform **three essential preprocessing steps**:\n",
    "\n",
    "### Step 1: Normalization (Scale Pixel Values)\n",
    "**What:** Convert pixel values from `[0, 255]` to `[0, 1]` by dividing by 255.0\n",
    "\n",
    "**Why:**\n",
    "- Neural networks train **faster and more stably** with small input values\n",
    "- Large pixel values (e.g., 255) can cause **gradient explosion**\n",
    "- Normalized inputs help **gradient descent converge** more reliably\n",
    "- All features are now on the **same scale**\n",
    "\n",
    "### Step 2: Flattening (2D â†’ 1D)\n",
    "**What:** Reshape each 28Ã—28 2D image into a 784-element 1D vector\n",
    "\n",
    "**Why:**\n",
    "- Dense (fully-connected) layers **require 1D input**\n",
    "- They cannot process 2D grids directly (CNNs can, but we're using Dense layers)\n",
    "- Flattening preserves all pixel values: 28 Ã— 28 = 784\n",
    "- Shape changes: `(60000, 28, 28)` â†’ `(60000, 784)`\n",
    "\n",
    "### Step 3: One-Hot Encoding (Labels)\n",
    "**What:** Convert integer labels (0-9) to binary vectors of length 10\n",
    "\n",
    "**Example:**\n",
    "- Label `3` (Dress) â†’ `[0, 0, 0, 1, 0, 0, 0, 0, 0, 0]`\n",
    "- Label `7` (Sneaker) â†’ `[0, 0, 0, 0, 0, 0, 0, 1, 0, 0]`\n",
    "\n",
    "**Why:**\n",
    "- **Categorical Crossentropy loss** expects this format\n",
    "- Matches the **10-neuron softmax output** layer\n",
    "- Treats all classes as **equally different** (no ordinal relationship)\n",
    "\n",
    "After preprocessing, our data is ready for training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "=============================================================================\n",
    "SECTION 5: DATA PREPROCESSING\n",
    "=============================================================================\n",
    "Three preprocessing steps prepare the raw data for the neural network:\n",
    "\n",
    "Step 1 - NORMALIZATION: Scale pixel values from [0, 255] to [0, 1]\n",
    "  â†’ Neural networks train faster and more stably with small input values\n",
    "  â†’ Formula: normalized_pixel = original_pixel / 255.0\n",
    "\n",
    "Step 2 - FLATTENING: Reshape 2D images (28x28) to 1D vectors (784)\n",
    "  â†’ Dense (fully-connected) layers require 1D input vectors\n",
    "  â†’ Each image becomes a single row of 784 features (28 Ã— 28 = 784)\n",
    "\n",
    "Step 3 - ONE-HOT ENCODING: Convert integer labels to binary vectors\n",
    "  â†’ Example: label 3 (Dress) â†’ [0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
    "  â†’ Required by Categorical Crossentropy loss function\n",
    "  â†’ Matches the 10-neuron softmax output layer\n",
    "\"\"\"\n",
    "\n",
    "# --- Step 1: NORMALIZATION ---\n",
    "# Convert pixel values from integers [0, 255] to floats [0.0, 1.0]\n",
    "# This scaling helps gradient descent converge faster and prevents\n",
    "# large pixel values from dominating the learning process\n",
    "X_train = X_train_raw.astype('float32') / 255.0\n",
    "X_test = X_test_raw.astype('float32') / 255.0\n",
    "print(\"Step 1 - Normalization complete\")\n",
    "print(f\"  Pixel value range: [{X_train.min()}, {X_train.max()}]\")\n",
    "\n",
    "# --- Step 2: FLATTENING ---\n",
    "# Reshape each 28x28 image into a 1D vector of 784 values\n",
    "# Dense layers expect 1D input: (samples, features) not (samples, height, width)\n",
    "X_train = X_train.reshape(-1, 784)   # -1 means \"infer number of samples\"\n",
    "X_test = X_test.reshape(-1, 784)\n",
    "print(f\"\\nStep 2 - Flattening complete\")\n",
    "print(f\"  X_train shape: {X_train.shape}\")   # (60000, 784)\n",
    "print(f\"  X_test shape:  {X_test.shape}\")     # (10000, 784)\n",
    "\n",
    "# --- Step 3: ONE-HOT ENCODING ---\n",
    "# Convert integer labels to one-hot encoded vectors\n",
    "# Example: label 3 â†’ [0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
    "# This is required because our output layer has 10 neurons with softmax,\n",
    "# and Categorical Crossentropy loss expects this format\n",
    "y_train = to_categorical(y_train_raw, num_classes=10)\n",
    "y_test = to_categorical(y_test_raw, num_classes=10)\n",
    "print(f\"\\nStep 3 - One-Hot Encoding complete\")\n",
    "print(f\"  y_train shape: {y_train.shape}\")   # (60000, 10)\n",
    "print(f\"  y_test shape:  {y_test.shape}\")     # (10000, 10)\n",
    "print(f\"  Example - label {y_train_raw[0]} encoded as: {y_train[0]}\")\n",
    "\n",
    "# --- Summary ---\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PREPROCESSING SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Training data:  {X_train.shape[0]} samples, {X_train.shape[1]} features each\")\n",
    "print(f\"Test data:      {X_test.shape[0]} samples, {X_test.shape[1]} features each\")\n",
    "print(f\"Label encoding: {y_train.shape[1]} classes (one-hot)\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 7: Neural Network Architecture - Design & Explanation\n",
    "\n",
    "### Architecture Overview\n",
    "We use a **Fully Connected (Dense) Neural Network** with the following structure:\n",
    "\n",
    "```\n",
    "Input (784) â†’ Dense(512, ReLU) â†’ Dropout(0.2) â†’ Dense(256, ReLU) â†’ Dropout(0.2) â†’ Dense(10, Softmax)\n",
    "```\n",
    "\n",
    "### Why Fully Connected (Dense) Layers?\n",
    "1. **Simplicity**: Dense networks are straightforward to understand and implement, making them ideal for learning fundamental neural network concepts\n",
    "2. **Fashion MNIST characteristics**: The images are small (28Ã—28), grayscale, and centered - dense layers can effectively learn patterns from these simple images\n",
    "3. **Sufficient performance**: Dense networks achieve 85-90% accuracy on Fashion MNIST, meeting our target\n",
    "4. **Educational value**: Understanding dense networks is the foundation before moving to more complex architectures like CNNs\n",
    "\n",
    "### Why NOT Convolutional Neural Networks (CNNs)?\n",
    "While CNNs would achieve slightly higher accuracy (~92-95%) by preserving spatial relationships, our goal is to demonstrate core neural network concepts. Dense layers process all pixels equally, which is sufficient for this small, centered dataset.\n",
    "\n",
    "### Layer-by-Layer Explanation\n",
    "\n",
    "| Layer | Output Shape | Parameters | Purpose |\n",
    "|-------|-------------|------------|----------|\n",
    "| Input | (784,) | 0 | Flattened 28Ã—28 image |\n",
    "| Dense 1 | (512,) | 401,920 | Learn low-level patterns (edges, textures) |\n",
    "| Dropout 1 | (512,) | 0 | Prevent overfitting (drop 20% of neurons) |\n",
    "| Dense 2 | (256,) | 131,328 | Combine patterns into higher-level features |\n",
    "| Dropout 2 | (256,) | 0 | Additional overfitting prevention |\n",
    "| Output | (10,) | 2,570 | Probability for each clothing class |\n",
    "\n",
    "### Activation Functions\n",
    "- **ReLU (Rectified Linear Unit)** for hidden layers: f(x) = max(0, x)\n",
    "  - Solves the vanishing gradient problem (unlike sigmoid/tanh)\n",
    "  - Computationally efficient\n",
    "  - Introduces non-linearity so the network can learn complex patterns\n",
    "  \n",
    "- **Softmax** for output layer: converts raw scores to probabilities that sum to 1.0\n",
    "  - Each of the 10 outputs represents the probability of that clothing class\n",
    "  - The class with the highest probability is the prediction\n",
    "\n",
    "### Dropout Regularization\n",
    "- Randomly disables 20% of neurons during each training step\n",
    "- Forces the network to not rely on any single neuron\n",
    "- Improves generalization to unseen data (reduces overfitting)\n",
    "\n",
    "### Neuron Count Rationale (512 â†’ 256 â†’ 10)\n",
    "- **Decreasing width** creates a \"funnel\" that compresses information\n",
    "- **512 neurons** in the first layer capture many low-level features from 784 inputs\n",
    "- **256 neurons** in the second layer combine these into fewer, more meaningful features\n",
    "- **10 neurons** in the output produce one score per clothing class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "=============================================================================\n",
    "SECTION 6: NEURAL NETWORK ARCHITECTURE\n",
    "=============================================================================\n",
    "Build a Fully Connected (Dense) neural network for clothing classification.\n",
    "\n",
    "Architecture: Input(784) â†’ Dense(512,ReLU) â†’ Dropout(0.2) â†’ Dense(256,ReLU) â†’ Dropout(0.2) â†’ Dense(10,Softmax)\n",
    "\n",
    "Why this architecture?\n",
    "- Dense layers are effective for small, centered images like Fashion MNIST\n",
    "- ReLU activation solves vanishing gradient and is computationally efficient\n",
    "- Softmax output produces probability distribution over 10 classes\n",
    "- Dropout prevents overfitting by randomly disabling 20% of neurons\n",
    "- Decreasing layer sizes (512â†’256â†’10) create hierarchical feature learning\n",
    "\"\"\"\n",
    "\n",
    "# Build the Sequential model\n",
    "# Sequential means layers are stacked one after another in order\n",
    "model = Sequential([\n",
    "    \n",
    "    # --- Hidden Layer 1: 512 neurons with ReLU activation ---\n",
    "    # Input: 784 features (flattened 28x28 image)\n",
    "    # 512 neurons learn to detect low-level patterns like edges and textures\n",
    "    # ReLU activation: f(x) = max(0, x) - introduces non-linearity\n",
    "    # Parameters: 784 * 512 + 512 (bias) = 401,920\n",
    "    Dense(512, activation='relu', input_shape=(784,), name='hidden_layer_1'),\n",
    "    \n",
    "    # --- Dropout Layer 1: 20% dropout rate ---\n",
    "    # Randomly sets 20% of neuron outputs to zero during training\n",
    "    # This prevents the network from becoming too dependent on specific neurons\n",
    "    # Improves generalization to unseen data (reduces overfitting)\n",
    "    # Note: Dropout is only active during training, not during prediction\n",
    "    Dropout(0.2, name='dropout_1'),\n",
    "    \n",
    "    # --- Hidden Layer 2: 256 neurons with ReLU activation ---\n",
    "    # Combines the 512 features from layer 1 into 256 higher-level features\n",
    "    # Learns more abstract patterns by combining low-level features\n",
    "    # Parameters: 512 * 256 + 256 (bias) = 131,328\n",
    "    Dense(256, activation='relu', name='hidden_layer_2'),\n",
    "    \n",
    "    # --- Dropout Layer 2: 20% dropout rate ---\n",
    "    # Additional regularization to prevent overfitting\n",
    "    Dropout(0.2, name='dropout_2'),\n",
    "    \n",
    "    # --- Output Layer: 10 neurons with Softmax activation ---\n",
    "    # One neuron per clothing class (10 total)\n",
    "    # Softmax converts raw scores into probabilities that sum to 1.0\n",
    "    # The class with the highest probability is the predicted class\n",
    "    # Parameters: 256 * 10 + 10 (bias) = 2,570\n",
    "    Dense(10, activation='softmax', name='output_layer')\n",
    "    \n",
    "], name='fashion_mnist_classifier')\n",
    "\n",
    "# Display model architecture summary\n",
    "# Shows layer names, output shapes, and parameter counts\n",
    "print(\"=\"*60)\n",
    "print(\"MODEL ARCHITECTURE SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "model.summary()\n",
    "\n",
    "# Calculate total trainable parameters\n",
    "total_params = model.count_params()\n",
    "print(f\"\\nTotal trainable parameters: {total_params:,}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 8: Loss Function & Optimizer Configuration\n",
    "\n",
    "### Loss Function: Categorical Crossentropy\n",
    "\n",
    "**What it is:** Measures how different the predicted probability distribution is from the true label. It penalizes the model more when it is confidently wrong.\n",
    "\n",
    "**Formula:** Loss = -Î£(y_true Ã— log(y_pred))\n",
    "\n",
    "**Why chosen:**\n",
    "1. **Multi-class classification**: Fashion MNIST has 10 mutually exclusive classes - Categorical Crossentropy is the standard loss for this type of problem\n",
    "2. **One-hot encoded labels**: Our labels are in one-hot format `[0,0,0,1,0,...]` which matches this loss function's expected input\n",
    "3. **Softmax pairing**: Mathematically pairs with softmax activation, providing clean gradients for efficient backpropagation\n",
    "4. **Penalizes confidence errors**: A prediction of 0.01 for the correct class is penalized much more than 0.4\n",
    "\n",
    "**Alternative considered:** Sparse Categorical Crossentropy - works with integer labels (0-9) instead of one-hot encoded. Functionally identical but we chose Categorical since we already one-hot encoded our labels.\n",
    "\n",
    "### Optimizer: Adam (Adaptive Moment Estimation)\n",
    "\n",
    "**What it is:** An advanced optimization algorithm that combines the benefits of two other optimizers - AdaGrad (adapts learning rate per parameter) and RMSProp (uses moving average of squared gradients).\n",
    "\n",
    "**Why chosen:**\n",
    "1. **Adaptive learning rates**: Automatically adjusts the learning rate for each parameter individually - parameters that update frequently get smaller rates, rare parameters get larger rates\n",
    "2. **Fast convergence**: Typically requires fewer epochs than basic SGD (Stochastic Gradient Descent)\n",
    "3. **Minimal tuning**: Works excellently with the default learning rate of 0.001\n",
    "4. **Momentum**: Uses exponential moving averages of gradients, which helps escape local minima and smooths the optimization path\n",
    "5. **Industry standard**: The most widely used optimizer in modern deep learning\n",
    "\n",
    "**Alternatives considered:**\n",
    "- **SGD**: Simpler but requires careful learning rate tuning and more epochs\n",
    "- **RMSProp**: Good but Adam generally performs better by adding momentum\n",
    "- **AdaGrad**: Learning rate can decay too aggressively over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "=============================================================================\n",
    "SECTION 7: LOSS FUNCTION AND OPTIMIZER CONFIGURATION\n",
    "=============================================================================\n",
    "\n",
    "LOSS FUNCTION: Categorical Crossentropy\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "- Measures difference between predicted probabilities and true one-hot labels\n",
    "- Formula: Loss = -Î£(y_true Ã— log(y_pred))\n",
    "- Standard choice for multi-class classification with one-hot encoded labels\n",
    "- Penalizes confident wrong predictions heavily\n",
    "- Pairs mathematically with softmax activation for clean gradients\n",
    "\n",
    "OPTIMIZER: Adam (Adaptive Moment Estimation)\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "- Combines benefits of RMSprop (adaptive learning rates) and Momentum\n",
    "- Automatically adjusts learning rate per parameter\n",
    "- Default learning rate: 0.001 (works well without tuning)\n",
    "- Fast convergence - typically needs fewer epochs than SGD\n",
    "- Most widely used optimizer in deep learning\n",
    "\n",
    "METRIC: Accuracy\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "- Percentage of correctly classified images\n",
    "- Simple, intuitive measure of model performance\n",
    "\"\"\"\n",
    "\n",
    "# Compile the model - this configures the training process\n",
    "# Three key components:\n",
    "#   1. optimizer - HOW to update weights (Adam)\n",
    "#   2. loss - WHAT to minimize (Categorical Crossentropy)\n",
    "#   3. metrics - WHAT to monitor (accuracy)\n",
    "model.compile(\n",
    "    optimizer='adam',                    # Adam optimizer with default lr=0.001\n",
    "    loss='categorical_crossentropy',    # Loss for multi-class one-hot labels\n",
    "    metrics=['accuracy']                # Track accuracy during training\n",
    ")\n",
    "\n",
    "print(\"Model compiled successfully!\")\n",
    "print(f\"  Optimizer: Adam (learning_rate=0.001)\")\n",
    "print(f\"  Loss function: Categorical Crossentropy\")\n",
    "print(f\"  Metrics: Accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 9: Model Training\n",
    "\n",
    "Now we train our neural network on the preprocessed Fashion MNIST data.\n",
    "\n",
    "### Training Parameters Explained\n",
    "\n",
    "**batch_size=128**\n",
    "- Process **128 images at a time** before updating weights\n",
    "- Larger batches = faster training but may reduce generalization\n",
    "- Smaller batches = slower but potentially better generalization\n",
    "- 128 is a good balance between speed and learning quality\n",
    "- With 60,000 training images: 60,000 / 128 = **469 batches per epoch**\n",
    "\n",
    "**epochs=20**\n",
    "- Complete **20 full passes** through the entire training dataset\n",
    "- Each epoch, the model sees all 60,000 training images\n",
    "- More epochs = more learning, but also risk of overfitting\n",
    "- 20 epochs is typically sufficient for convergence on Fashion MNIST\n",
    "\n",
    "**validation_split=0.2**\n",
    "- Reserve **20% of training data** (12,000 images) for validation\n",
    "- Validation data is **NOT used for training** - it monitors how well the model generalizes\n",
    "- This gives us three datasets:\n",
    "  - **Training**: 48,000 images (80% of train set) - used to update weights\n",
    "  - **Validation**: 12,000 images (20% of train set) - monitor overfitting\n",
    "  - **Test**: 10,000 images (held out completely) - final evaluation\n",
    "\n",
    "### What Happens During Training?\n",
    "\n",
    "For each epoch:\n",
    "1. **Forward pass**: Feed training images through the network, get predictions\n",
    "2. **Calculate loss**: Compare predictions to true labels using Categorical Crossentropy\n",
    "3. **Backward pass**: Calculate gradients (how much each weight contributed to the error)\n",
    "4. **Update weights**: Adam optimizer adjusts weights to reduce loss\n",
    "5. **Validation**: Evaluate on validation set (no weight updates)\n",
    "6. **Repeat** for next epoch\n",
    "\n",
    "You'll see a progress bar showing training and validation loss/accuracy for each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "=============================================================================\n",
    "SECTION 8: MODEL TRAINING\n",
    "=============================================================================\n",
    "Train the neural network on the preprocessed Fashion MNIST training data.\n",
    "\n",
    "Training Parameters:\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "- batch_size=128: Process 128 images at a time before updating weights.\n",
    "  Larger batches = faster training but may reduce generalization.\n",
    "  128 is a good balance between speed and learning quality.\n",
    "\n",
    "- epochs=20: Complete 20 full passes through the entire training dataset.\n",
    "  Each epoch, the model sees all 60,000 training images.\n",
    "  20 epochs is typically sufficient for convergence on Fashion MNIST.\n",
    "\n",
    "- validation_split=0.2: Reserve 20% of training data (12,000 images) for\n",
    "  validation. This data is NOT used for training - it monitors how well\n",
    "  the model generalizes to unseen data during training.\n",
    "  Training: 48,000 images | Validation: 12,000 images | Test: 10,000 images\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"STARTING MODEL TRAINING\")\n",
    "print(\"=\"*60)\n",
    "print(f\"  Training samples:    {int(X_train.shape[0] * 0.8):,} (80% of train set)\")\n",
    "print(f\"  Validation samples:  {int(X_train.shape[0] * 0.2):,} (20% of train set)\")\n",
    "print(f\"  Test samples:        {X_test.shape[0]:,} (held out for final evaluation)\")\n",
    "print(f\"  Batch size:          128\")\n",
    "print(f\"  Epochs:              20\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Train the model\n",
    "# model.fit() returns a History object containing loss and metric values per epoch\n",
    "history = model.fit(\n",
    "    X_train,                # Training images (60000, 784)\n",
    "    y_train,                # Training labels (60000, 10) - one-hot encoded\n",
    "    batch_size=128,         # Number of samples per gradient update\n",
    "    epochs=20,              # Number of complete passes through training data\n",
    "    validation_split=0.2,   # Fraction of training data to use for validation\n",
    "    verbose=1               # Show progress bar for each epoch\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING COMPLETED!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 10: Learning Curves - Loss & Accuracy Graphs\n",
    "\n",
    "Visualizing training history helps us understand **how well the model learned** and **whether it's overfitting**.\n",
    "\n",
    "### How to Interpret These Graphs\n",
    "\n",
    "#### Loss Graph (Left)\n",
    "- **Both lines should decrease** over epochs â†’ model is learning\n",
    "- **Training loss continues decreasing while validation loss increases** â†’ **OVERFITTING** (model memorizes training data)\n",
    "- **Both lines plateau** â†’ model has **CONVERGED** (learned what it can)\n",
    "- **Loss still decreasing at epoch 20** â†’ more epochs might improve performance\n",
    "- **Green vertical line** marks the epoch with minimum validation loss (best generalization)\n",
    "\n",
    "#### Accuracy Graph (Right)\n",
    "- **Both lines should increase** over epochs\n",
    "- **Gap between training and validation accuracy** indicates overfitting\n",
    "- **Small gap (<5%)** is normal and acceptable\n",
    "- **Large gap (>10%)** suggests overfitting - model performs much better on training data than unseen data\n",
    "\n",
    "### What to Look For\n",
    "- **Ideal scenario**: Both training and validation metrics improve together and plateau\n",
    "- **Underfitting**: High loss, low accuracy on both training and validation (model hasn't learned enough)\n",
    "- **Overfitting**: Training metrics great, validation metrics poor (model memorized instead of learned)\n",
    "- **Good fit**: Small gap between training and validation, both metrics good\n",
    "\n",
    "### Our Target\n",
    "We aim for **85-90% test accuracy** with minimal overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "=============================================================================\n",
    "SECTION 9: TRAINING VISUALIZATION - LEARNING CURVES\n",
    "=============================================================================\n",
    "Plot the training and validation loss/accuracy over epochs.\n",
    "\n",
    "How to interpret these graphs:\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "LOSS GRAPH (left):\n",
    "  - Both lines should decrease over epochs (model is learning)\n",
    "  - If validation loss starts increasing while training loss continues\n",
    "    decreasing â†’ OVERFITTING (model memorizes training data)\n",
    "  - If both lines plateau â†’ model has CONVERGED (learned what it can)\n",
    "  - If loss is still decreasing â†’ more epochs might help\n",
    "\n",
    "ACCURACY GRAPH (right):\n",
    "  - Both lines should increase over epochs\n",
    "  - Gap between training and validation accuracy indicates overfitting\n",
    "  - A small gap (<5%) is normal and acceptable\n",
    "\"\"\"\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "fig.suptitle('Model Training History', fontsize=16, fontweight='bold')\n",
    "\n",
    "# --- Plot 1: Loss over Epochs ---\n",
    "axes[0].plot(history.history['loss'], label='Training Loss', \n",
    "             color='blue', linewidth=2)\n",
    "axes[0].plot(history.history['val_loss'], label='Validation Loss', \n",
    "             color='red', linewidth=2, linestyle='--')\n",
    "axes[0].set_title('Loss Over Epochs', fontsize=14)\n",
    "axes[0].set_xlabel('Epoch', fontsize=12)\n",
    "axes[0].set_ylabel('Loss (Categorical Crossentropy)', fontsize=12)\n",
    "axes[0].legend(fontsize=11)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "# Mark the epoch with minimum validation loss\n",
    "min_val_loss_epoch = np.argmin(history.history['val_loss'])\n",
    "min_val_loss = min(history.history['val_loss'])\n",
    "axes[0].axvline(x=min_val_loss_epoch, color='green', linestyle=':', alpha=0.7)\n",
    "axes[0].annotate(f'Min val loss: {min_val_loss:.4f}\\n(epoch {min_val_loss_epoch+1})',\n",
    "                xy=(min_val_loss_epoch, min_val_loss),\n",
    "                xytext=(min_val_loss_epoch+2, min_val_loss+0.05),\n",
    "                arrowprops=dict(arrowstyle='->', color='green'),\n",
    "                fontsize=10, color='green')\n",
    "\n",
    "# --- Plot 2: Accuracy over Epochs ---\n",
    "axes[1].plot(history.history['accuracy'], label='Training Accuracy', \n",
    "             color='blue', linewidth=2)\n",
    "axes[1].plot(history.history['val_accuracy'], label='Validation Accuracy', \n",
    "             color='red', linewidth=2, linestyle='--')\n",
    "axes[1].set_title('Accuracy Over Epochs', fontsize=14)\n",
    "axes[1].set_xlabel('Epoch', fontsize=12)\n",
    "axes[1].set_ylabel('Accuracy', fontsize=12)\n",
    "axes[1].legend(fontsize=11)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print final metrics\n",
    "print(f\"\\nFinal Training Loss:      {history.history['loss'][-1]:.4f}\")\n",
    "print(f\"Final Validation Loss:    {history.history['val_loss'][-1]:.4f}\")\n",
    "print(f\"Final Training Accuracy:  {history.history['accuracy'][-1]:.4f} ({history.history['accuracy'][-1]*100:.2f}%)\")\n",
    "print(f\"Final Validation Accuracy:{history.history['val_accuracy'][-1]:.4f} ({history.history['val_accuracy'][-1]*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 11: Model Evaluation on Test Set\n",
    "\n",
    "The **true test** of our model is how well it performs on the **test set** - 10,000 images that were completely held out during training.\n",
    "\n",
    "### Why the Test Set Matters\n",
    "\n",
    "1. **Completely unseen data**: The model has NEVER seen these images during training or validation\n",
    "2. **Unbiased evaluation**: Gives us an honest estimate of real-world performance\n",
    "3. **Prevents data leakage**: Ensures we didn't accidentally \"cheat\" by tuning on validation data\n",
    "4. **Simulates production**: Mimics how the model will perform on new images in deployment\n",
    "\n",
    "### What We're Evaluating\n",
    "\n",
    "- **Test Loss**: How well the model's probability predictions match the true labels\n",
    "- **Test Accuracy**: Percentage of correctly classified images (our main metric)\n",
    "\n",
    "### Our Target\n",
    "\n",
    "- **Goal**: 85%+ accuracy on test set\n",
    "- **Excellent**: 90%+ accuracy\n",
    "- **State-of-the-art** (with CNNs): 92-95% accuracy\n",
    "\n",
    "### Next Steps After Evaluation\n",
    "\n",
    "We'll also generate predictions on the entire test set to create a **confusion matrix**, which shows exactly which clothing types the model confuses most often."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "=============================================================================\n",
    "SECTION 10: MODEL EVALUATION ON TEST SET\n",
    "=============================================================================\n",
    "Evaluate the trained model on the test set (10,000 images never seen during training).\n",
    "This gives us an unbiased estimate of how well our model will perform on new data.\n",
    "\n",
    "The test set was kept completely separate during training - the model has\n",
    "never learned from these images, making this a fair evaluation.\n",
    "\"\"\"\n",
    "\n",
    "# Evaluate on test set\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"TEST SET EVALUATION RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"  Test Loss:     {test_loss:.4f}\")\n",
    "print(f\"  Test Accuracy: {test_accuracy:.4f} ({test_accuracy*100:.2f}%)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if test_accuracy >= 0.85:\n",
    "    print(f\"\\nâœ“ Target accuracy (>85%) ACHIEVED!\")\n",
    "else:\n",
    "    print(f\"\\nâš  Target accuracy (>85%) not met. Consider tuning hyperparameters.\")\n",
    "\n",
    "# Generate predictions for confusion matrix\n",
    "# model.predict() returns probability arrays for each sample\n",
    "y_pred_probs = model.predict(X_test, verbose=0)\n",
    "\n",
    "# Convert probability arrays to class indices using argmax\n",
    "# argmax returns the index of the highest probability = predicted class\n",
    "y_pred_classes = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "# Convert one-hot encoded test labels back to class indices for comparison\n",
    "y_true_classes = np.argmax(y_test, axis=1)\n",
    "\n",
    "print(f\"\\nPredictions generated for {len(y_pred_classes)} test images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 12: Confusion Matrix (10Ã—10)\n",
    "\n",
    "The **confusion matrix** is one of the most insightful visualizations for classification models. It shows exactly which classes the model confuses.\n",
    "\n",
    "### How to Read the Confusion Matrix\n",
    "\n",
    "- **ROWS** represent the **TRUE (actual) labels** - what the clothing item really is\n",
    "- **COLUMNS** represent the **PREDICTED labels** - what the model thinks it is\n",
    "- **DIAGONAL cells** (top-left to bottom-right) = **CORRECT predictions** âœ“\n",
    "- **OFF-DIAGONAL cells** = **MISCLASSIFICATIONS** âœ—\n",
    "\n",
    "### Example Interpretation\n",
    "\n",
    "- Cell `[row=Shirt, col=T-shirt]` = \"Shirts that were incorrectly predicted as T-shirts\"\n",
    "- Cell `[row=Sneaker, col=Sneaker]` = \"Sneakers correctly identified as Sneakers\"\n",
    "- High diagonal values = good performance\n",
    "- High off-diagonal values = common confusion patterns\n",
    "\n",
    "### What to Look For\n",
    "\n",
    "1. **Strong diagonal** - indicates overall good performance\n",
    "2. **Weak spots** - classes with lower diagonal values need improvement\n",
    "3. **Confusion patterns** - which classes are commonly mistaken for each other?\n",
    "   - Example: Shirts vs T-shirts (similar appearance)\n",
    "   - Example: Pullover vs Coat (both outerwear)\n",
    "   - Example: Sneaker vs Ankle boot (both footwear)\n",
    "\n",
    "### Perfect Model\n",
    "\n",
    "A perfect model would have:\n",
    "- **All values on the diagonal** (all predictions correct)\n",
    "- **Zeros everywhere else** (no misclassifications)\n",
    "\n",
    "In practice, some confusion is inevitable due to the complexity and similarity of certain clothing types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "=============================================================================\n",
    "SECTION 11: CONFUSION MATRIX (10Ã—10)\n",
    "=============================================================================\n",
    "The confusion matrix shows how well the model classifies each clothing type.\n",
    "\n",
    "How to read it:\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "- ROWS represent the TRUE (actual) labels\n",
    "- COLUMNS represent the PREDICTED labels\n",
    "- DIAGONAL cells (top-left to bottom-right) = CORRECT predictions\n",
    "- OFF-DIAGONAL cells = MISCLASSIFICATIONS\n",
    "- Example: If cell [row=Shirt, col=T-shirt] has a high value,\n",
    "  it means the model often mistakes Shirts for T-shirts\n",
    "\n",
    "A perfect model would have values ONLY on the diagonal (zero errors).\n",
    "\"\"\"\n",
    "\n",
    "# Compute the 10x10 confusion matrix\n",
    "cm = confusion_matrix(y_true_classes, y_pred_classes)\n",
    "\n",
    "# Create a large, detailed heatmap visualization\n",
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "# Use seaborn heatmap for professional visualization\n",
    "sns.heatmap(\n",
    "    cm,\n",
    "    annot=True,          # Show numbers in each cell\n",
    "    fmt='d',             # Integer format (not scientific notation)\n",
    "    cmap='Blues',         # Blue color gradient\n",
    "    xticklabels=class_names,  # Column labels = class names\n",
    "    yticklabels=class_names,  # Row labels = class names\n",
    "    linewidths=0.5,      # Grid line width\n",
    "    linecolor='gray',    # Grid line color\n",
    "    square=True          # Make cells square\n",
    ")\n",
    "\n",
    "plt.title('Confusion Matrix - Fashion MNIST Classification\\n(10Ã—10: True vs Predicted Labels)', \n",
    "          fontsize=16, fontweight='bold', pad=20)\n",
    "plt.xlabel('Predicted Label', fontsize=13, labelpad=10)\n",
    "plt.ylabel('True Label', fontsize=13, labelpad=10)\n",
    "plt.xticks(rotation=45, ha='right', fontsize=10)\n",
    "plt.yticks(rotation=0, fontsize=10)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print per-class accuracy from the confusion matrix\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PER-CLASS ACCURACY (from Confusion Matrix)\")\n",
    "print(\"=\"*60)\n",
    "for i, name in enumerate(class_names):\n",
    "    # Accuracy for class i = correct predictions / total samples of class i\n",
    "    class_accuracy = cm[i, i] / cm[i].sum()\n",
    "    correct = cm[i, i]\n",
    "    total = cm[i].sum()\n",
    "    bar = 'â–ˆ' * int(class_accuracy * 30) + 'â–‘' * (30 - int(class_accuracy * 30))\n",
    "    print(f\"  {name:15s}: {bar} {class_accuracy:.2%}  ({correct}/{total})\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Identify most confused pairs\n",
    "print(\"\\nMost Common Misclassifications:\")\n",
    "# Zero out diagonal to find off-diagonal maximums\n",
    "cm_no_diag = cm.copy()\n",
    "np.fill_diagonal(cm_no_diag, 0)\n",
    "for _ in range(5):\n",
    "    idx = np.unravel_index(cm_no_diag.argmax(), cm_no_diag.shape)\n",
    "    true_class = class_names[idx[0]]\n",
    "    pred_class = class_names[idx[1]]\n",
    "    count = cm_no_diag[idx]\n",
    "    print(f\"  {true_class} â†’ predicted as {pred_class}: {count} times\")\n",
    "    cm_no_diag[idx] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 13: Predict Unknown Image (User-Loaded External Images)\n",
    "\n",
    "Now let's use our trained model to classify **your own clothing images**!\n",
    "\n",
    "### How This Function Works\n",
    "\n",
    "The `predict_clothing_image()` function takes any external image file and:\n",
    "\n",
    "1. **Loads the image** from the file path\n",
    "2. **Converts to grayscale** (if it's a color image)\n",
    "3. **Resizes to 28Ã—28 pixels** (Fashion MNIST format)\n",
    "4. **Auto-inverts colors if needed** â€” Fashion MNIST has light clothing on dark backgrounds, but real photos are the opposite. The function detects this and inverts automatically.\n",
    "5. **Normalizes pixel values** to [0, 1] (divide by 255)\n",
    "6. **Flattens to 784-element vector** (same as training data)\n",
    "7. **Feeds through the neural network**\n",
    "8. **Returns the predicted class and confidence**\n",
    "\n",
    "### Color Inversion â€” Why It Matters\n",
    "\n",
    "Fashion MNIST images look like this: **black background, light-colored clothing silhouette**. Real-world photos are the opposite: **light/white background, dark clothing**. Without inverting, the model gets completely different input than what it was trained on, causing poor predictions. The function handles this automatically.\n",
    "\n",
    "### Important Notes\n",
    "\n",
    "**Image Format Requirements:**\n",
    "- Supported formats: PNG, JPG, JPEG, BMP, WEBP\n",
    "- Any size image works (auto-resized to 28Ã—28)\n",
    "- Color or grayscale images both work (auto-converted)\n",
    "\n",
    "### How to Use\n",
    "\n",
    "**Option 1 â€” File Upload Widget** (recommended for Jupyter):\n",
    "Run the upload widget cell below, click \"Upload\", select your clothing image, and get an instant prediction.\n",
    "\n",
    "**Option 2 â€” Manual Path Input**:\n",
    "When prompted, type or paste the full path to your clothing image file.\n",
    "\n",
    "### Visualization Output\n",
    "\n",
    "The function displays:\n",
    "- **Left plot**: Original image as loaded\n",
    "- **Center plot**: The preprocessed 28Ã—28 grayscale image (what the model sees)\n",
    "- **Right plot**: Confidence bar chart for all 10 classes\n",
    "- **Top 3 predictions** with confidence percentages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "=============================================================================\n",
    "SECTION 12: UNKNOWN IMAGE PREDICTION (User-Loaded External Images)\n",
    "=============================================================================\n",
    "This function loads an external clothing image provided by the user and\n",
    "predicts its class using our trained neural network.\n",
    "\n",
    "The image goes through the same preprocessing pipeline as training data:\n",
    "  1. Convert to grayscale (if color)\n",
    "  2. Resize to 28x28 pixels\n",
    "  3. Auto-invert colors if needed (Fashion MNIST = light-on-dark,\n",
    "     real photos = dark-on-light). Detected by mean pixel value.\n",
    "  4. Normalize pixel values to [0, 1]\n",
    "  5. Flatten to 784-element vector\n",
    "  6. Reshape to (1, 784) for model input (batch of 1)\n",
    "\"\"\"\n",
    "\n",
    "def predict_clothing_image(image_path, model, class_names):\n",
    "    \"\"\"\n",
    "    Load an external clothing image and predict its class using the trained model.\n",
    "    \n",
    "    Preprocesses the image to match Fashion MNIST format: grayscale, 28x28 pixels,\n",
    "    light-on-dark colors, normalized, and flattened. Automatically inverts colors\n",
    "    for real-world photos that have dark clothing on light backgrounds.\n",
    "    \n",
    "    Args:\n",
    "        image_path (str): Path to the clothing image file (PNG, JPG, JPEG, BMP, WEBP)\n",
    "        model: Trained Keras model\n",
    "        class_names (list): List of 10 class name strings\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (predicted_class_name, confidence_score) or (None, 0.0) on error\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Step 1: Load the image file\n",
    "        img = Image.open(image_path)\n",
    "        print(f\"Image loaded: {image_path}\")\n",
    "        print(f\"  Original size: {img.size}, Mode: {img.mode}\")\n",
    "        \n",
    "        # Keep a copy of the original for display\n",
    "        img_original = img.copy()\n",
    "        \n",
    "        # Step 2: Convert to grayscale (single channel)\n",
    "        # Fashion MNIST images are grayscale, so we need to match that format\n",
    "        img_gray = img.convert('L')\n",
    "        \n",
    "        # Step 3: Resize to 28x28 pixels (Fashion MNIST image dimensions)\n",
    "        img_resized = img_gray.resize((28, 28))\n",
    "        \n",
    "        # Step 4: Convert to numpy array for numerical processing\n",
    "        img_array = np.array(img_resized)\n",
    "        \n",
    "        # Step 5: Auto-detect and invert colors if needed\n",
    "        # Fashion MNIST has BLACK background (0) with LIGHT clothing (~255)\n",
    "        # Real-world photos typically have LIGHT/WHITE background with DARK clothing\n",
    "        # If mean pixel value > 127, the background is light â†’ invert colors\n",
    "        mean_pixel = img_array.mean()\n",
    "        if mean_pixel > 127:\n",
    "            img_array = 255 - img_array\n",
    "            print(f\"  Color inverted (mean pixel {mean_pixel:.0f} > 127 â†’ light background detected)\")\n",
    "        else:\n",
    "            print(f\"  No inversion needed (mean pixel {mean_pixel:.0f} â‰¤ 127 â†’ dark background)\")\n",
    "        \n",
    "        # Step 6: Normalize pixel values from [0, 255] to [0, 1]\n",
    "        img_normalized = img_array.astype('float32') / 255.0\n",
    "        \n",
    "        # Step 7: Flatten from 2D (28, 28) to 1D (784,)\n",
    "        # Then reshape to (1, 784) - batch dimension required by model\n",
    "        img_flat = img_normalized.reshape(1, 784)\n",
    "        \n",
    "        # Step 8: Make prediction using the trained model\n",
    "        prediction = model.predict(img_flat, verbose=0)\n",
    "        \n",
    "        # Step 9: Extract predicted class and confidence\n",
    "        predicted_class_idx = np.argmax(prediction[0])\n",
    "        confidence = prediction[0][predicted_class_idx]\n",
    "        predicted_name = class_names[predicted_class_idx]\n",
    "        \n",
    "        # --- Display results ---\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "        \n",
    "        # Left: Show original image as loaded\n",
    "        if img_original.mode == 'L':\n",
    "            axes[0].imshow(np.array(img_original), cmap='gray')\n",
    "        else:\n",
    "            axes[0].imshow(np.array(img_original))\n",
    "        axes[0].set_title('Original Image', fontsize=12)\n",
    "        axes[0].axis('off')\n",
    "        \n",
    "        # Center: Show preprocessed 28x28 image (what the model sees)\n",
    "        axes[1].imshow(img_array, cmap='gray')\n",
    "        axes[1].set_title('Preprocessed 28Ã—28\\n(Model Input)', fontsize=12)\n",
    "        axes[1].axis('off')\n",
    "        \n",
    "        # Right: Show prediction confidence for all classes\n",
    "        colors = ['green' if i == predicted_class_idx else 'steelblue' \n",
    "                  for i in range(10)]\n",
    "        axes[2].barh(class_names, prediction[0], color=colors)\n",
    "        axes[2].set_title('Prediction Confidence', fontsize=12)\n",
    "        axes[2].set_xlim([0, 1])\n",
    "        axes[2].set_xlabel('Probability')\n",
    "        \n",
    "        fig.suptitle(f'Prediction: {predicted_name} ({confidence:.1%} confidence)', \n",
    "                     fontsize=14, fontweight='bold')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(f\"\\n{'='*40}\")\n",
    "        print(f\"  Predicted Class: {predicted_name}\")\n",
    "        print(f\"  Confidence:      {confidence:.2%}\")\n",
    "        print(f\"{'='*40}\")\n",
    "        \n",
    "        # Show top 3 predictions\n",
    "        top3_idx = np.argsort(prediction[0])[::-1][:3]\n",
    "        print(\"\\nTop 3 predictions:\")\n",
    "        for rank, idx in enumerate(top3_idx, 1):\n",
    "            print(f\"  {rank}. {class_names[idx]:15s}: {prediction[0][idx]:.2%}\")\n",
    "        \n",
    "        return predicted_name, float(confidence)\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\"ERROR: Image file not found at '{image_path}'\")\n",
    "        print(\"  Please check the file path and try again.\")\n",
    "        return None, 0.0\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Could not process image - {str(e)}\")\n",
    "        return None, 0.0\n",
    "\n",
    "\n",
    "# === QUICK TEST: Predict on a sample from the test set ===\n",
    "print(\"Quick test: predicting on a sample from the test set...\\n\")\n",
    "sample_idx = 0\n",
    "sample_img = Image.fromarray(X_test_raw[sample_idx])\n",
    "sample_path = 'sample_test_image.png'\n",
    "sample_img.save(sample_path)\n",
    "print(f\"Saved test image (true label: {class_names[y_test_raw[sample_idx]]}) to '{sample_path}'\")\n",
    "\n",
    "predicted_class, confidence = predict_clothing_image(sample_path, model, class_names)\n",
    "print(f\"\\nTrue label: {class_names[y_test_raw[sample_idx]]}\")\n",
    "print(f\"Predicted:  {predicted_class}\")\n",
    "print(f\"Correct:    {'YES' if predicted_class == class_names[y_test_raw[sample_idx]] else 'NO'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "djyepdh12y5",
   "source": [
    "### Try It Yourself â€” Upload Your Own Clothing Image\n",
    "\n",
    "Run the cell below to upload and classify your own clothing images.\n",
    "\n",
    "**How it works:**\n",
    "1. Click the **\"Choose Files\"** button that appears\n",
    "2. Select one or more clothing image files from your computer\n",
    "3. The model will automatically predict each uploaded image\n",
    "\n",
    "**Tips for best results:**\n",
    "- Use images with a **clean, single-color background**\n",
    "- The image should contain **one clothing item**\n",
    "- Any size and format (PNG, JPG, JPEG, BMP, WEBP) works â€” it will be auto-resized\n",
    "- Colors are **auto-inverted** to match Fashion MNIST format\n",
    "\n",
    "**Run the cell again** to upload more images."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "nwj3ldnevp9",
   "source": [
    "# SECTION 12b: UPLOAD & PREDICT using Google Colab File Upload\n",
    "from google.colab import files\n",
    "import io\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"UPLOAD YOUR CLOTHING IMAGES\")\n",
    "print(\"=\" * 60)\n",
    "print(\"Click Choose Files below to select images from your computer.\")\n",
    "print()\n",
    "\n",
    "uploaded = files.upload()\n",
    "\n",
    "if uploaded:\n",
    "    print(str(len(uploaded)) + \" file(s) uploaded. Running predictions...\")\n",
    "    print()\n",
    "    for filename, file_data in uploaded.items():\n",
    "        print(\"=\" * 60)\n",
    "        print(\"Processing: \" + filename)\n",
    "        print(\"=\" * 60)\n",
    "        try:\n",
    "            img = Image.open(io.BytesIO(file_data))\n",
    "            print(\"  Original size: \" + str(img.size) + \", Mode: \" + img.mode)\n",
    "            img_original = img.copy()\n",
    "            img_gray = img.convert('L')\n",
    "            img_resized = img_gray.resize((28, 28))\n",
    "            img_array = np.array(img_resized)\n",
    "            mean_pixel = img_array.mean()\n",
    "            if mean_pixel > 127:\n",
    "                img_array = 255 - img_array\n",
    "                print(\"  Color inverted (light background detected)\")\n",
    "            else:\n",
    "                print(\"  No inversion needed (dark background)\")\n",
    "            img_normalized = img_array.astype('float32') / 255.0\n",
    "            img_flat = img_normalized.reshape(1, 784)\n",
    "            prediction = model.predict(img_flat, verbose=0)\n",
    "            predicted_class_idx = np.argmax(prediction[0])\n",
    "            confidence = prediction[0][predicted_class_idx]\n",
    "            predicted_name = class_names[predicted_class_idx]\n",
    "            fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "            if img_original.mode == 'L':\n",
    "                axes[0].imshow(np.array(img_original), cmap='gray')\n",
    "            else:\n",
    "                axes[0].imshow(np.array(img_original))\n",
    "            axes[0].set_title('Original Image', fontsize=12)\n",
    "            axes[0].axis('off')\n",
    "            axes[1].imshow(img_array, cmap='gray')\n",
    "            axes[1].set_title('Preprocessed 28x28', fontsize=12)\n",
    "            axes[1].axis('off')\n",
    "            bar_colors = ['green' if i == predicted_class_idx else 'steelblue' for i in range(10)]\n",
    "            axes[2].barh(class_names, prediction[0], color=bar_colors)\n",
    "            axes[2].set_title('Prediction Confidence', fontsize=12)\n",
    "            axes[2].set_xlim([0, 1])\n",
    "            axes[2].set_xlabel('Probability')\n",
    "            pct = str(round(confidence * 100, 1))\n",
    "            fig.suptitle('Prediction: ' + predicted_name + ' (' + pct + '%)', fontsize=14, fontweight='bold')\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            top3_idx = np.argsort(prediction[0])[::-1][:3]\n",
    "            print(\"  Predicted: \" + predicted_name + \" (\" + pct + \"%)\")\n",
    "            print(\"  Top 3:\")\n",
    "            for rank, idx in enumerate(top3_idx, 1):\n",
    "                p = str(round(prediction[0][idx] * 100, 2))\n",
    "                print(\"    \" + str(rank) + \". \" + class_names[idx] + \": \" + p + \"%\")\n",
    "            print()\n",
    "        except Exception as e:\n",
    "            print(\"  ERROR: Could not process \" + filename + \" - \" + str(e))\n",
    "            print()\n",
    "else:\n",
    "    print(\"No files uploaded.\")\n",
    "\n",
    "print(\"Done! Run this cell again to upload more images.\")\n"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 14: Save Trained Model (Optional)\n",
    "\n",
    "After spending time training our model, we want to **save it for future use** without needing to retrain.\n",
    "\n",
    "### What Gets Saved\n",
    "\n",
    "The `.keras` file format (TensorFlow 2.x) saves the **complete model**:\n",
    "\n",
    "1. **Architecture** - layer structure, neuron counts, activations\n",
    "2. **Weights** - all learned parameters from training\n",
    "3. **Optimizer state** - Adam optimizer configuration and momentum\n",
    "4. **Compilation settings** - loss function, metrics\n",
    "\n",
    "### Benefits of Saving the Model\n",
    "\n",
    "- **Skip retraining**: Load the model instantly instead of training for 20 epochs\n",
    "- **Deployment**: Use the model in production applications\n",
    "- **Sharing**: Share the trained model with others\n",
    "- **Versioning**: Save different versions as you experiment\n",
    "- **Continued training**: Load and continue training with more data\n",
    "\n",
    "### How to Load the Model Later\n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load the complete model\n",
    "loaded_model = tf.keras.models.load_model('fashion_mnist_model.keras')\n",
    "\n",
    "# Use it for predictions\n",
    "predict_clothing_image('my_image.jpg', loaded_model, class_names)\n",
    "```\n",
    "\n",
    "### Alternative Save Formats\n",
    "\n",
    "- **HDF5 format** (legacy): `model.save('model.h5')`\n",
    "- **Weights only**: `model.save_weights('weights.h5')` - saves only weights, not architecture\n",
    "- **SavedModel format**: `model.save('saved_model/')` - for TensorFlow Serving (production)\n",
    "\n",
    "We recommend the `.keras` format as it's the modern standard and most convenient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "=============================================================================\n",
    "SECTION 13: SAVE TRAINED MODEL (Optional)\n",
    "=============================================================================\n",
    "Save the trained model to disk so it can be loaded later for predictions\n",
    "without needing to retrain. This is useful for deployment or continued work.\n",
    "\"\"\"\n",
    "\n",
    "# Save the complete model (architecture + weights + optimizer state)\n",
    "model.save('fashion_mnist_model.keras')\n",
    "print(\"Model saved to 'fashion_mnist_model.keras'\")\n",
    "print(\"\\nTo load the model later:\")\n",
    "print(\"  loaded_model = tf.keras.models.load_model('fashion_mnist_model.keras')\")\n",
    "print(\"  predict_clothing_image('image.jpg', loaded_model, class_names)\")"
   ]
  }
 ]
}